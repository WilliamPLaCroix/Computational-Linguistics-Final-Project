{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Function\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN implementation and architecture closely follows the tutorial at: https://www.kaggle.com/code/purvasingh/text-generation-via-rnn-and-lstms-pytorch#Pre-processing-Stock-News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Check if a GPU is available for training\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Data parameters\n",
    "sequence_length = 10  # The number of characters in a sequence, \"10-gram\"?\n",
    "batch_size = 128  # The batch size for training the model.\n",
    "\n",
    "# Training parameters\n",
    "number_epochs = 10  # The number of times to iterate over the entire dataset during training.\n",
    "learning_rate = 0.001  # The rate at which the model adjusts its parameters during training.\n",
    "\n",
    "# Model parameters\n",
    "embedding_dimension = 200  # The number of dimensions in the embedding layer.\n",
    "hidden_dimension = 250  # The number of hidden units in each LSTM layer.\n",
    "number_layers = 2  # The number of LSTM layers in the model.\n",
    "show_every_n_batches = 100  # How often to print training statistics, such as loss and accuracy, during training. Set to every 100 batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Dictionaries for special characters in the text\n",
    "# >>> Not really necessary any more with the GPT HSK corpus.\n",
    "SPECIAL_WORDS: dict = {'PADDING': '<PAD>'}\n",
    "\n",
    "punctuation_dict: dict = {\"。\": \"<PERIOD>\",\n",
    "                        \"．\": \"<DECIMAL>\",\n",
    "                        \"，\": \"<COMMA>\",\n",
    "                        \"！\": \"<EXCLAMATION>\",\n",
    "                        \"）\": \"<RIGHT_PARENTHESIS>\",\n",
    "                        \"（\": \"<LEFT_PARENTHESIS>\",\n",
    "                        '\"': \"<QUOTE>\",\n",
    "                        \"”\": \"<RIGHT_QUOTE>\",\n",
    "                        \"“\": \"<LEFT_QUOTE>\",\n",
    "                        \"？\": \"<QUESTION_MARK>\",\n",
    "                        \"：\": \"<COLON>\",\n",
    "                        \"；\": \"<SEMICOLON>\",\n",
    "                        \"》\": \"<RIGHT_BRACKETS>\",\n",
    "                        \"《\": \"<LEFT_BRACKETS>\",\n",
    "                        \"‘\": \"<RIGHT_APOSTROPHE>\",\n",
    "                        \"’\": \"<LEFT_APOSTROPHE>\",\n",
    "                        \"\\t\": \"<TAB>\",\n",
    "                        \"\\n\": \"<NEW_LINE>\",\n",
    "                        \"＊\": \"<ASTERISK>\",\n",
    "                        \"%\": \"<PERCENT>\",\n",
    "                        \"＄\": \"<DOLLAR_SIGN>\",\n",
    "                        \"＆\": \"<AMPRISAND>\"\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A recurrent neural network class that inherits from the PyTorch nn.Module class.\n",
    "\n",
    "    Args:\n",
    "        vocabulary_size (int): number of unique characters in the vocabulary.\n",
    "        output_size (int): size of the output, which is equal to the vocabulary size.\n",
    "        embedding_dimension (int): size of the feature vector for mapping characters.\n",
    "        hidden_dimension (int): number of nodes in each hidden layer.\n",
    "        number_layers (int): number of layers in the LSTM model.\n",
    "        dropout (float, optional): dropout rate. Default is 0.5.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (nn.Embedding): embedding layer that maps characters to feature vectors.\n",
    "        lstm (nn.LSTM): LSTM layer that processes the embedded characters.\n",
    "        vocabulary_size (int): number of unique characters in the vocabulary.\n",
    "        output_size (int): size of the output, which is equal to the vocabulary size.\n",
    "        embedding_dimension (int):size of the feature vector for mapping characters.\n",
    "        hidden_dimension (int): number of nodes in each hidden layer.\n",
    "        number_layers (int): number of layers in the LSTM model.\n",
    "        fully_connected_layer (nn.Linear): fully connected layer that produces the final output.\n",
    "\n",
    "    Methods:\n",
    "        forward(input, hidden_state): Forward pass of the network.\n",
    "        init_hidden(batch_size): Initialize the hidden state of the LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocabulary_size, output_size, embedding_dimension, hidden_dimension, number_layers, dropout=0.5):\n",
    "        super().__init__() # call __init__ method of parent class to inherit its attributes\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension) # create an embedding layer\n",
    "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension, number_layers, dropout=dropout, batch_first=True) # create an LSTM layer\n",
    "        self.vocabulary_size = vocabulary_size # set the number of unique characters in the vocabulary\n",
    "        self.output_size = output_size # set the output size (which is equal to the vocabulary size)\n",
    "        self.embedding_dimension = embedding_dimension # set the size of the feature vector for mapping characters\n",
    "        self.hidden_dimension = hidden_dimension # set the number of nodes in each hidden layer\n",
    "        self.number_layers = number_layers # set the number of layers in the LSTM model\n",
    "        self.fully_connected_layer = nn.Linear(hidden_dimension, output_size) # create a fully connected layer\n",
    "    \n",
    "    def forward(self, input, hidden_state):\n",
    "        batch_size = input.size(0) # get the batch size\n",
    "        input = input.long() # convert the input to a LongTensor\n",
    "        embedddings = self.embedding(input) # apply the embedding layer to the input\n",
    "        lstm_out, hidden_state = self.lstm(embedddings, hidden_state) # apply the LSTM layer to the embeddings and the hidden state\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dimension) # stack LSTM outputs\n",
    "        output = self.fully_connected_layer(lstm_out) # apply the fully connected layer to the LSTM outputs\n",
    "        output = output.view(batch_size, -1, self.output_size) # reshape the output to batch_size * sequence_length * output_size\n",
    "        output = output[:, -1] # get the final batch\n",
    "        return output, hidden_state # return the final batch word scores and the final hidden state\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # create two new zero tensors of size number_layers * batch_size * hidden_dimension\n",
    "        # initialize hidden state with zero weights, and run on GPU if possible\n",
    "        weights = next(self.parameters()).data # get the weights of the model\n",
    "        if(train_on_gpu) == True: # if we are training on a GPU\n",
    "            # create a tuple of two tensors, both of size number_layers * batch_size * hidden_dimension, and initialize them with zero weights, and run them on the GPU if available\n",
    "            hidden_state = (weights.new(self.number_layers, batch_size, self.hidden_dimension).zero_().cuda(), \n",
    "                     weights.new(self.number_layers, batch_size, self.hidden_dimension).zero_().cuda()) \n",
    "        else:\n",
    "            hidden_state = (weights.new(self.number_layers, batch_size, self.hidden_dimension).zero_(),\n",
    "                     weights.new(self.number_layers, batch_size, self.hidden_dimension).zero_()) \n",
    "        return hidden_state # return the hidden state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename):\n",
    "    \"\"\"\n",
    "    Preprocesses data for use in a language model.\n",
    "\n",
    "    Args: filename (str):name of the file to preprocess.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Read in the data from the file.\n",
    "    with open(f\"./data/{filename}.txt\", \"r\", encoding=\"GBK\") as file_in:\n",
    "        data = file_in.readlines()\n",
    "    # Strip whitespace from the data.\n",
    "    data = [line.strip() for line in data]\n",
    "    # Join all the lines into one big long string.\n",
    "    long_data = ''.join(data)\n",
    "    # Count the occurrences of each character in the data.\n",
    "    character_counts = Counter(long_data)\n",
    "    # Filter out any punctuation characters (for some reason?).\n",
    "    trimmed_counts = {item: count for item, count in character_counts.items() if item not in punctuation_dict.keys()}\n",
    "    # Sort the remaining characters in descending order of frequency.\n",
    "    sorted_characters = sorted(trimmed_counts, key=trimmed_counts.get, reverse=True)\n",
    "    # Create dictionaries to convert between characters and their indices in the sorted list.\n",
    "    index_to_character = {index: character for index, character in enumerate(sorted_characters + list(SPECIAL_WORDS.values()))}\n",
    "    character_to_index = {character: index for index, character in index_to_character.items()}\n",
    "    # Convert the long string of data into a list of indices.\n",
    "    indexed_data = list()\n",
    "    for character in long_data:\n",
    "        try:\n",
    "            indexed_data.append(character_to_index[character])\n",
    "        except KeyError:\n",
    "            # If a character is not in the vocabulary, skip it.\n",
    "            pass\n",
    "    # Save the preprocessed data and dictionaries using pickle.\n",
    "    pickle.dump((indexed_data, character_to_index, index_to_character, punctuation_dict), open(f\"./data/save/{filename}_preprocess.p\", 'wb'))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(characters, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batches input/target pairs for a given set of characters.\n",
    "    returns a DataLoader object that can be used to iterate over the batches during training or testing.\n",
    "    \n",
    "    Args:\n",
    "        characters (list): list of characters to be used as input for the RNN.\n",
    "        sequence_length (int): length of each input sequence.\n",
    "        batch_size (int): number of input/target pairs per batch.\n",
    "    \n",
    "    Returns: DataLoader: PyTorch DataLoader object that contains batches of input/target pairs.\n",
    "    \"\"\"\n",
    "    # Calculate the number of batches to make\n",
    "    number_batches = len(characters)//batch_size\n",
    "    # Only consider characters up to the last full batch\n",
    "    characters = characters[:number_batches * batch_size]\n",
    "    # Initialize empty lists to hold inputs and targets\n",
    "    x, y = [], []\n",
    "\n",
    "    # Iterate through the characters, creating input/target pairs\n",
    "    for i in range(0, len(characters)-sequence_length):\n",
    "        # Get the starting and ending indices for the input sequence\n",
    "        i_end = i + sequence_length        \n",
    "        # Extract the input sequence and append it to the list of inputs\n",
    "        batch_x = characters[i: i + sequence_length]\n",
    "        x.append(batch_x)\n",
    "        # Extract the target character and append it to the list of targets\n",
    "        batch_y = characters[i_end]\n",
    "        y.append(batch_y)\n",
    "    \n",
    "    # Create a TensorDataset from the input/target pairs\n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    # Use DataLoader to create batches from the TensorDataset\n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "        \n",
    "    return data_loader # Return the DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(rnn, optimizer, criterion, input_batch, target, hidden):\n",
    "    \"\"\"Runs forward and backward propagation on a given batch of input data and target labels.\n",
    "\n",
    "    Args:\n",
    "        rnn (torch.nn.Module): RNN model to use for the forward propagation.\n",
    "        optimizer (torch.optim.Optimizer): optimizer to use for the backward propagation.\n",
    "        criterion (torch.nn.modules.loss._Loss): loss function to use for computing the error.\n",
    "        input_batch (torch.Tensor): input batch of data with shape (batch_size, seq_len).\n",
    "        target (torch.Tensor): target batch of labels with shape (batch_size, seq_len).\n",
    "        hidden (torch.Tensor): hidden state of the RNN with shape (num_layers * num_directions, batch_size, hidden_size).\n",
    "\n",
    "    Returns: tuple containing the loss over the batch and the new hidden state of the RNN with shape (num_layers * num_directions, batch_size, hidden_size).\n",
    "    \"\"\"\n",
    "    # Move data to GPU, if available\n",
    "    if(train_on_gpu) == True:\n",
    "        rnn.cuda()\n",
    "    # Creating variables for hidden state to prevent back-propagation\n",
    "    hidden_states = tuple([state.data for state in hidden]) # Historical states \n",
    "    rnn.zero_grad()\n",
    "    # Move inputs and targets to GPU\n",
    "    inputs, targets = input_batch.cuda(), target.type(torch.LongTensor).cuda() \n",
    "    # Get the output and new hidden state from the RNN model\n",
    "    output, hidden_states = rnn(inputs, hidden_states)\n",
    "    # Compute the loss between output and target\n",
    "    loss = criterion(output, targets)\n",
    "    # Perform backpropagation and optimization\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5) # Gradient clipping to prevent exploding gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(rnn, batch_size, optimizer, criterion, number_epochs, show_every_n_batches=100) -> RNN:\n",
    "    \"\"\"\n",
    "    trains a given RNN model for a specified number of epochs using a specified optimizer and criterion. \n",
    "    It also prints the training loss every show_every_n_batches batches.\n",
    "\n",
    "    Parameters:\n",
    "        rnn (nn.Module): RNN model to be trained\n",
    "        batch_size (int): batch size for training\n",
    "        optimizer (torch.optim.Optimizer): optimizer to use for training the model\n",
    "        criterion (torch.nn.modules.loss._Loss): loss criterion to use for training the model\n",
    "        number_epochs (int): number of epochs to train the model\n",
    "        show_every_n_batches (int, optional): number of batches after which to print the training loss. Default is 100.\n",
    "    \n",
    "    Returns:\n",
    "        rnn (nn.Module): trained RNN model\n",
    "    \"\"\"\n",
    "\n",
    "    batch_losses = [] # initialize an empty list to store the losses for each batch\n",
    "    rnn.train() # set the model to training mode\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % number_epochs) # print the number of epochs for training\n",
    "    for epoch_i in range(1, number_epochs + 1): # loop over the epochs\n",
    "        hidden = rnn.init_hidden(batch_size) # initialize the hidden state\n",
    "        for batch_j, (inputs, labels) in enumerate(train_loader, 1): # loop over the batches in the training data\n",
    "            number_batches = len(train_loader.dataset)//batch_size # calculate the total number of batches\n",
    "            if batch_j > number_batches: # if the current batch number is greater than the total number of batches, break out of the loop\n",
    "                break\n",
    "            # forward + backward propagation\n",
    "            loss, hidden = forward_backward_propagation(rnn, optimizer, criterion, inputs, labels, hidden) # perform forward and backward propagation on the current batch          \n",
    "            batch_losses.append(loss) # record the loss for the current batch\n",
    "            if batch_j % show_every_n_batches == 0: # if the current batch number is a multiple of show_every_n_batches, print the average loss for the previous batches\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, number_epochs, np.average(batch_losses)))\n",
    "                batch_losses = [] # reset the batch loss list for the next batches\n",
    "   \n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(rnn, prime_id, index_to_character, punctuation_dict, pad_value, predict_len=100) -> str:\n",
    "    \"\"\"\n",
    "    Generates a sequence of characters using a trained RNN model.\n",
    "\n",
    "    Args:\n",
    "        rnn (nn.Module): trained RNN model.\n",
    "        prime_id (int): index of the character to start the sequence with.\n",
    "        index_to_character (dict): dictionary that maps character indices to characters.\n",
    "        punctuation_dict (dict): dictionary that maps punctuation characters to their corresponding token.\n",
    "        pad_value (int): value to use for padding the sequence.\n",
    "        predict_len (int): number of characters to generate (default is 100).\n",
    "\n",
    "    Returns: string containing the generated characters.\n",
    "    \"\"\"\n",
    "    rnn.eval() # set the model to evaluation mode\n",
    "\n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_sequence = np.full((1, sequence_length), pad_value) # initialize the current sequence with the pad_value\n",
    "    current_sequence[-1][-1] = prime_id # set the last element of the sequence to the prime_id\n",
    "    predicted = [index_to_character[prime_id]] # initialize the predicted list with the character corresponding to the prime_id\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu == True:\n",
    "            current_sequence = torch.LongTensor(current_sequence).cuda() # move the current sequence to the GPU if available\n",
    "        else:\n",
    "            current_sequence = torch.LongTensor(current_sequence) # convert the current sequence to a tensor\n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_sequence.size(0))\n",
    "        # get the output of the rnn\n",
    "        output, hidden_state = rnn(current_sequence, hidden)\n",
    "        # get the next word probabilities\n",
    "        probability = Function.softmax(output, dim=1).data\n",
    "        if train_on_gpu == True:\n",
    "            probability = probability.cpu() # move the probability tensor to the CPU if available\n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        probability, top_i = probability.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        # select the likely next word index with some element of randomness\n",
    "        probability = probability.numpy().squeeze()\n",
    "        # choose the next character index based on the probability distribution\n",
    "        character_i = np.random.choice(top_i, p=probability/probability.sum())\n",
    "        # retrieve that character from the dictionary\n",
    "        character = index_to_character[character_i]\n",
    "        predicted.append(character)     \n",
    "        # the generated character becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_sequence = np.roll(current_sequence.cpu(), -1, 1)\n",
    "        current_sequence[-1][-1] = character_i\n",
    "    \n",
    "    gen_sentences = ''.join(predicted) # concatenate the predicted characters into a string\n",
    "    \n",
    "    return gen_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    \"\"\"\n",
    "    Loads preprocessed data from a file, or preprocesses the data if the file doesn't exist.\n",
    "\n",
    "    Args: filename (str): The name of the file to load or create.\n",
    "\n",
    "    Returns: tuple containing the following:\n",
    "        - indexed_data (list): list of integers representing the preprocessed text data.\n",
    "        - character_to_index (dict): dictionary that maps characters to their corresponding indices.\n",
    "        - index_to_character (dict): dictionary that maps indices to their corresponding characters.\n",
    "        - punctuation_dict (dict): dictionary that maps punctuation characters to their corresponding token.\n",
    "    \"\"\"\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Try to load preprocessed data from file\n",
    "            indexed_data, character_to_index, index_to_character, punctuation_dict = pickle.load(open(f\"./data/save/{filename}_preprocess.p\", mode='rb'))\n",
    "            return indexed_data, character_to_index, index_to_character, punctuation_dict\n",
    "        except FileNotFoundError:\n",
    "            # If the file doesn't exist, preprocess the data and try again\n",
    "            print(\"Couldn't find file, rebuilding data...\")\n",
    "            preprocess_data(filename)  # This is assumed to be a function that preprocesses the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(filename, finetune=False):\n",
    "    \"\"\"\n",
    "    Train a RNN model on the preprocessed data and save the trained model to a file.\n",
    "\n",
    "    Args: filename (str): filename to use for the saved model file.\n",
    "\n",
    "    Returns: print statement confirming success.\n",
    "    \"\"\"\n",
    "    # initailize vocab and output sizes for training \n",
    "    vocabulary_size = len(character_to_index)\n",
    "    # Output size\n",
    "    output_size = vocabulary_size\n",
    "    # Create an RNN model with the specified architecture\n",
    "    if finetune == False:\n",
    "        rnn = RNN(vocabulary_size, output_size, embedding_dimension, hidden_dimension, number_layers, dropout=0.5)\n",
    "    elif finetune == True:\n",
    "        rnn = torch.load('./data/save/full_hsk_sentences_trained_rnn.pt')\n",
    "    \n",
    "    if train_on_gpu:\n",
    "        rnn.cuda() # Move the model to the GPU if available\n",
    "\n",
    "    # Define the loss and optimization functions for training\n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Train the RNN model\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, number_epochs, show_every_n_batches)\n",
    "    # Save the trained model to a file\n",
    "    torch.save(trained_rnn, f'./data/save/{filename}_trained_rnn.pt')\n",
    "    return print('Model Trained and Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_hsk_sentences loaded.\n"
     ]
    }
   ],
   "source": [
    "# Define the filename of the preprocessed data to be loaded and trained on\n",
    "filename = f\"full_hsk_sentences\"\n",
    " # Load the preprocessed data if available or preprocess if not\n",
    "indexed_data, character_to_index, index_to_character, punctuation_dict = load(filename)\n",
    "\n",
    "# Train baseline rnn model\n",
    "while True:\n",
    "        # Try to load preprocessed data from file\n",
    "        try:\n",
    "            trained_rnn = torch.load(f'./data/save/{filename}_trained_rnn.pt')\n",
    "            print(f\"{filename} loaded.\")\n",
    "            break\n",
    "        # If the file doesn't exist, train the model and try again\n",
    "        except FileNotFoundError:\n",
    "            # Prepare data by batching it into sequence_length chunks of batch_size\n",
    "            train_loader = batch_data(indexed_data, sequence_length, batch_size)\n",
    "            # Train RNN on the prepared data and save the trained model to file\n",
    "            train_and_save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:04<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters generated for HSK 0 model: 61\n",
      "Generated text based on HSK 0 model: \n",
      "我们可以去公园散步他很\n",
      "你想吃什么他喜欢吃水果\n",
      "他是一个医生我会说一点\n",
      "她你有没有兄弟姐妹你喜\n",
      "这是你的铅笔吗请问你有\n",
      "人我们可以去公园玩儿你\n",
      "的你喜欢吃蔬菜吗我想要\n",
      "是你住在哪里我们的学生\n",
      "那我们可以坐地铁去吗我\n",
      "一你喜欢吃甜食你喜欢旅\n",
      "Unique characters generated for HSK 1 model: 61\n",
      "Generated text based on HSK 1 model: \n",
      "我今天天气很好我们去公\n",
      "你觉都很便宜我叫李明他\n",
      "他很漂亮你要不要来看看\n",
      "她我天美你要不要试试你\n",
      "这我很漂天这个菜很热闹\n",
      "人天我们可以在图书馆学\n",
      "的我们要坐地铁去你觉得\n",
      "是我的狗很可爱我们可以\n",
      "那我们可以在那家书店买\n",
      "一了你喜欢吃中国菜吗这\n",
      "Unique characters generated for HSK 2 model: 62\n",
      "Generated text based on HSK 2 model: \n",
      "我的学校有很多俱乐部比\n",
      "你身有名这个地方很热闹\n",
      "他有一个很好的福利制度\n",
      "她妈样看电影我喜欢听音\n",
      "这个地方很舒适他的中文\n",
      "人身体有这个周末天气预\n",
      "的身高他们在商场购物你\n",
      "是身试试样我们可以在这\n",
      "那有一个很好的福利制度\n",
      "一名吗这个地方很舒适他\n",
      "Unique characters generated for HSK 3 model: 61\n",
      "Generated text based on HSK 3 model: \n",
      "我们可以在这里等一会儿\n",
      "你要不非常努力我的爷爷\n",
      "他道的对音乐会们个房间\n",
      "她对让了我的家离学校不\n",
      "这道这个项目我们每个月\n",
      "人们多个问题我们需要更\n",
      "的对道菜的味道很好我很\n",
      "是春十我们需要更加注重\n",
      "那道菜对让我们一起工作\n",
      "一么个问题我们可以在这\n",
      "Unique characters generated for HSK 4 model: 78\n",
      "Generated text based on HSK 4 model: \n",
      "我们要始终保持谦虚谨慎\n",
      "你保取市的文化遗产非常\n",
      "他是个非常勤奋的学生总\n",
      "她的成就令人钦佩是许多\n",
      "这个公司的业务范围非常\n",
      "人年们要保持积极的心态\n",
      "的支持这个项目需要一个\n",
      "是支与与外国内流畅交流\n",
      "那年道德高工作效率我们\n",
      "一个完整的管理体系她在\n",
      "Unique characters generated for HSK 5 model: 69\n",
      "Generated text based on HSK 5 model: \n",
      "我们需要保持积极的心态\n",
      "你人们的魅力和财强这个\n",
      "他有魅深受它这个城市的\n",
      "她非常重要才能更好地开\n",
      "这个有识的权果非常重要\n",
      "人们的重要这个项目需要\n",
      "的本异以在这个城市的交\n",
      "是护精深受启行这个项目\n",
      "那挑益被风用他的演讲让\n",
      "一护环境和野生动物我们\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_length = 10  # number of characters to generate\n",
    "\n",
    "# Initialize empty list to store generated texts\n",
    "text_collection = []\n",
    "# Loop through each of the 6 HSK levels\n",
    "for i in tqdm(range(1, 7)):\n",
    "    # Load preprocessed data and the trained RNN model for the current HSK level\n",
    "    filename = f\"hsk{i}_sentences_filtered\"\n",
    "    # Load preprocessed data from file\n",
    "    unused_indexed_data, vocab_to_int, int_to_vocab, punctuation_dict = load(filename)\n",
    "    while True:\n",
    "        # Try to load preprocessed data from file\n",
    "        try:\n",
    "            trained_rnn = torch.load(f'./data/save/{filename}_trained_rnn.pt')\n",
    "            break\n",
    "        # If the file doesn't exist, train the model and try again\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Training new rnn for HSK {i}\")\n",
    "            # Create batches of training data\n",
    "            train_loader = batch_data(indexed_data, sequence_length, batch_size)\n",
    "            # Train model and save it to a file\n",
    "            train_and_save(filename, finetune=True)\n",
    "    # Set the prime words to use for each text\n",
    "    prime_words = ['我', '你', '他', '她', '这', '人', '的', '是', '那', '一']  # character(s) to use to start generation\n",
    "    pad_word = SPECIAL_WORDS['PADDING']\n",
    "\n",
    "    texts = [] # Initialize empty list to store text during generation\n",
    "    for prime_word in prime_words: # Generate text based on each prime word\n",
    "        # Generate text using the trained RNN and the current prime word\n",
    "        text = generate(trained_rnn, vocab_to_int[prime_word], int_to_vocab, punctuation_dict, vocab_to_int[pad_word], text_length)\n",
    "        texts.append(text)\n",
    "    # append generated texts to the list of generated texts\n",
    "    text_collection.append('\\n'.join(texts))\n",
    "\n",
    "# Print number of unique characters in each generated text as well as generated text\n",
    "for i, output in enumerate(text_collection):\n",
    "    print(f\"Unique characters generated for HSK {i} model: {len(set(output))}\")\n",
    "    print(f\"Generated text based on HSK {i} model: \")\n",
    "    print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
